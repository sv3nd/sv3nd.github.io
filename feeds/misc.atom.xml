<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Svend Vanderveken - misc</title><link href="https://sv3nd.github.io/" rel="alternate"></link><link href="https://sv3nd.github.io/feeds/misc.atom.xml" rel="self"></link><id>https://sv3nd.github.io/</id><updated>2017-12-19T00:00:00+01:00</updated><entry><title>A commented Kafka configuration</title><link href="https://sv3nd.github.io/a-commented-kafka-configuration.html" rel="alternate"></link><published>2017-12-19T00:00:00+01:00</published><updated>2017-12-19T00:00:00+01:00</updated><author><name>Svend Vanderveken</name></author><id>tag:sv3nd.github.io,2017-12-19:/a-commented-kafka-configuration.html</id><summary type="html">&lt;p&gt;Diving into Kafka configuration is a beautiful journey into its features. &lt;/p&gt;
&lt;p&gt;As a preparation for a production deployment of Kafka 0.11, I gathered a set of comments on what I think are some interesting parameters. All this amazing wisdom is mostly extracted from the few resources mentioned at the â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Diving into Kafka configuration is a beautiful journey into its features. &lt;/p&gt;
&lt;p&gt;As a preparation for a production deployment of Kafka 0.11, I gathered a set of comments on what I think are some interesting parameters. All this amazing wisdom is mostly extracted from the few resources mentioned at the end of this post. &lt;/p&gt;
&lt;h3&gt;A grain of salt...&lt;/h3&gt;
&lt;p&gt;This is all for information only, I honestly think most of the points below are relevant and correct, though mistakes and omissions are likely present here and there. &lt;/p&gt;
&lt;p&gt;You should not apply any of this blindly to your production environment and hope for anything to work. &lt;/p&gt;
&lt;p&gt;The wiser thing to do &lt;em&gt;of course&lt;/em&gt; is renting my services, I'm freelance, see my contact references beside :)  &lt;/p&gt;
&lt;h3&gt;Broker basic parameters&lt;/h3&gt;
&lt;p&gt;Let's start with a few comments on Kafka broker basic parameters, maybe located somewhere like &lt;code&gt;etc/kafka/server.properties&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;First off, each broker must know its enpoint as known by consumers and producers. This is because a Kafka cluster keeps a dynamic list of which broker serves which topic partition. Consumers and producers then obtain that routing information as part of the topic metadata and connect directly to the appropriate broker when exchanging data. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;listeners=PLAINTEXT://your.host.name:9092
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, it's a good idea to specify a &lt;code&gt;chroot&lt;/code&gt; folder in the zookeeper connection string to keep the future flexibility of sharing it with other tools or even another Kafka cluster. Recall that several Kafka brokers are considered to be part of the same cluster if they share the same location on a zookeeper ensemble. Zookeeper is super sensible to load and access latency, so sharing it betweeen many frameworks is not always a good idea.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;zookeeper.connect=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181/kafka
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While we're on the subject of zookeeper, the two following can be handy. Note that setting a long timeout is not a magic solution to latency issues since it makes the detection and resolution of crashed brokers slower.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;zookeeper.connection.timeout.ms=6000
zookeeper.session.timeout.ms=6000
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The broker log location can be specified as a comma-separated list of mount points. For higher throughput, one can specify several disks here.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;log.dirs=/some/location,/some/other-location-on-another-disk
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The following parameter specifies the number of threads used during startup and  shut-down for cleaning up logs and getting to a stable state. Since they are only used at that moment, increasing it may speedup startup time (especially right after a major failure that requires lots' of cleanup) and should otherwise not impact the performance during the rest of the lifetime of the broker.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;num.recovery.threads.per.data.dir=2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In production, I would disable topic auto-creation, to make sure all topics are created with explictly chosen parameters. I would also tend to disable the deletion of topics: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;auto.create.topics.enable=false

delete.topic.enable=false
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Default max message size is 1M. That setting can also be set per topic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# this is overridable at topic creation time with --config max.message.bytes
#message.max.bytes=1000000
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Data retention&lt;/h3&gt;
&lt;p&gt;Log retention is configured by time and/or by size. If both are specified, whichever condition is true first triggers a cleanup. Time-based retention can be specified in &lt;code&gt;hour&lt;/code&gt;, &lt;code&gt;minutes&lt;/code&gt; or &lt;code&gt;ms&lt;/code&gt;, you should only specify one of those time period, though if you specify several, the smallest time granularity takes precedence. &lt;/p&gt;
&lt;p&gt;Logs are sliced into segments of the max size or max duration specified in the last two paremeters below. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gotcha 1&lt;/strong&gt;: Kafka is not going to clean up less than a full and past segment. This means that if you have a low traffic topic and set its retention to, say, a couple of hours, data might still take days to be cleaned up since we need to fill up a segment before cleaning it up.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gotach 2&lt;/strong&gt;: relying on &lt;code&gt;log.segment.ms&lt;/code&gt; implies that segments of all topic partitions are going to be rolled at approximatively the same moment, which might impact all broker sensibly of you have lot's of partitions and data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# this can be overriden at topic creation time with --config retention.ms&lt;/span&gt;
log.retention.hours&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;168&lt;/span&gt;

&lt;span class="c1"&gt;# this can be overriden at topic creation time with --config retention.bytes&lt;/span&gt;
log.retention.bytes&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1073741824&lt;/span&gt;

&lt;span class="c1"&gt;# this is overridable at topic creation time with --config segment.bytes &lt;/span&gt;
log.segment.bytes&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;268435456&lt;/span&gt;

&lt;span class="c1"&gt;# this is overridable at topic creation time with --config segment.ms&lt;/span&gt;
log.segment.ms&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;123456&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Consumer have their offsets committed in Kafka now (except if your client handles them explicitly in some other way), so they are also subject to retention. The default is 1 day. If you have a low traffic topic that might receive less than one message per day, your consumers offsets would not get updated and could be removed from Kafka. Setting &lt;code&gt;offsets.retention.minutes&lt;/code&gt; to a higher value should help in such case. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# keep consumer offset for two weeks
offsets.retention.minutes=20160
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Broker data availability parameters&lt;/h3&gt;
&lt;p&gt;If a topic is replicated, all read and write operations are performed on the leader partition and all other replicas are just slave copies. Such slave replica is said to be "out-of-sync" if it lags behind the latest records available in its leader. &lt;/p&gt;
&lt;p&gt;In case the leader crashes at a moment when all live replicas are out-of-sync, Kafka will by default not allow such "unclean" replicas to become the new leader since data could be lost and/or consumers could be confused about offset fuzzy business. If you would like to favour availability over data consistency, you can choose to allow such "unclean leader election". Note that you can specify this per topic as well. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# this is overridable at topic creation time with --config unclean.leader.election.enable
unclean.leader.election.enable=false
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The following parameter is a similar availabiliy vs consistency tradeoff: data producers have the possibility to request that "all" partition replicas confirm the reception of a record before considering the write operation as successful (cf &lt;code&gt;acks&lt;/code&gt; parameter below). In case some replicas are known to be out-of-sync, we know they are not going to provide such acknowledgment at the moment. The parameter below specifies the minimum number of replicas that must still be in sync such that we can consider that "all" replicas have confirmed the reception of a record.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# this is overridable at topic creation time with --config min.insync.replicas
min.insync.replicas=2
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Kafka producers&lt;/h3&gt;
&lt;p&gt;Kafka producers and consumer are rich clients that are packed with features like batching, message routing, compression, retries... and all that gets to be parameterized as well :) &lt;/p&gt;
&lt;p&gt;One key piece of information to keep in mind is that configuring producers and consumers makes sense when we code directly against their API, &lt;strong&gt;as well when we want to configure Kafka Connect, Kafka Stream, Flink Kafka connector, Spark Kafka connector and pretty much any java or scala component that relies on them&lt;/strong&gt;, simply because, well, all their features still matter once they're wrapped in such tools.  &lt;/p&gt;
&lt;h4&gt;Basic parameters&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;bootstrap.servers&lt;/code&gt; should specify a couple of kafka brokers. If at least one of them is still valid at the moment the connection happens, the client will then rely on Kafka service discovery to figure out all the others. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bootstrap.servers=some-broker:9092.some-other-broker:9092
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Pretty much all a broker knows about a record payload is that it's a key/value pair made of bytes. Serializers are used by the producer to convert java instances to such byte arrays. One possible choice here is to rely on &lt;a href="https://github.com/confluentinc/schema-registry/blob/3.3.1-post/avro-serializer/src/main/java/io/confluent/kafka/serializers/KafkaAvroSerializer.java"&gt;Confluent's avro / schema registry serializer&lt;/a&gt; to obtain avro records with a schema properly declared and versioned in the Conluent schema registry.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;value.serializer=some.class

key.serializer=some.class
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Producers also optionally handle data compression for us. For maximum throughput, there is a tradeoff to be experimentally found between message size and cpu time spent (de)compressing it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# compression codec. 
# &amp;quot;none&amp;quot;, &amp;quot;snappy&amp;quot;, &amp;quot;lz4&amp;quot; or &amp;quot;gzip&amp;quot;
compression.type=lz4
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The following controls the maximum amount of time the client will wait for the response of a request.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;request.timeout.ms=30000
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Delivery guaranties&lt;/h4&gt;
&lt;p&gt;Kafka producers perform retries for us! As many as we want. The &lt;code&gt;retries&lt;/code&gt; parameter specifies the maximum amount of retries that will be attempted on a retry-able error (like, leader not available) and &lt;code&gt;retry.backoff.ms&lt;/code&gt; specifies how long to wait between each attempt. &lt;/p&gt;
&lt;p&gt;Note that as the producer keep on retrying while potentially also trying to send new traffic, pending messages can quickly occupy some space, so make sure &lt;code&gt;buffer.memory&lt;/code&gt; is set properly. Finally, once the memory buffer is full (or if topic metadata are impossible to obtain at the moment), the producer will block during &lt;code&gt;max.block.ms&lt;/code&gt; before blowing up. &lt;/p&gt;
&lt;p&gt;I guess this is Kafka's take on back-pressure. &lt;/p&gt;
&lt;p&gt;Really, if we care about data consistency, and assuming all upstreams components are behaving accordingly, blocking might be the best thing to do here. Well, blowing up might also be the way. Each case should be designed. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# this default to 0, unless you enabled idempotence
retries=2147483647

retry.backoff.ms=100

buffer.memory=33554432

max.block.ms=60000
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As mentioned above on the section &lt;code&gt;min.insync.replica&lt;/code&gt;, producer can specify the amount of required acknowledgments for a write to be considered successful. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# this defaults to 1, unless you enabled idempotence
acks=all
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Idempotent producers is one of the awesome feature that Kafka folks gifted to the world in version 0.11. That is a long subject, though in a nutshell it guarantees that successfully written records are written exactly once to the brokers. Previously, due to some corner cases in the retry mechanism, some message could have ended-up being duplicated. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Idempotent retries features of Kafka, introduced in 0.11, 
# Part of components enabing Kafka Streams exactly-once processing semantics.
enable.idempotence=false
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Output record batching&lt;/h4&gt;
&lt;p&gt;Kafka producers also automatically batch our records together and send them asynchronously!&lt;/p&gt;
&lt;p&gt;In case enough data is available when the producer sends data, it will pack them per batch of &lt;code&gt;batch.size&lt;/code&gt; bytes. If less data is available it just sends what it has without waiting, unless &lt;code&gt;linger.ms&lt;/code&gt; is set to a positive value, in which case it waits a bit to get a chance to pack a few more: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;batch.size=16384

linger.ms=0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;By default, the producer will wait for a batch to be acknowledged, as specified by &lt;code&gt;acks&lt;/code&gt; above, before sending the next batch. For potentially faster throughput, we set the following parameter to some value greater than one to specify the maximum amount of such un-acknowledged batches that we allow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gotcha&lt;/strong&gt;: setting this to anything else than 1 destroys the per-topic ordering guarantee of Kafka, simply because some in-flight batches might fail and be retried, while others might go through, in any order.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# If set enable.idempotence to true, it may not be greater than 5
#max.in.flight.requests.per.connection=1
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Kafka consummers&lt;/h3&gt;
&lt;h4&gt;Basic parameters&lt;/h4&gt;
&lt;p&gt;Kafka consumer group share the read load when reading from topics. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;group.id=&amp;quot;my client&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;See producer discussion regarding broker endpoint and serialisers&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bootstrap.servers=some-broker:9092.some-other-broker:9092

value.deserializer=some.class

key.deserializer=some.class
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;fetch.min.bytes=1&lt;/code&gt; means the consumer will start reading as early as at least one message of at least one byte is available. Forcing higher values here might lead to reading by larger chunks and relying on less network round trips. &lt;/p&gt;
&lt;p&gt;In a similar fashion, &lt;code&gt;max.poll.records&lt;/code&gt; specifies, well, the maximum number of message to fetch each time. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fetch.min.bytes=1

max.poll.records=500
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The following specifies where to start reading a topic when a consumer appears and does not have a previous offset to start from. This is the typical situation that happens when a new consumer group is created, although it can also be relevant if that offset existed in the past but disappeared due to the &lt;code&gt;offsets.retention.minutes&lt;/code&gt; parameter mentioned above. &lt;code&gt;latest&lt;/code&gt; will make the consumer tail the log while &lt;code&gt;earliest&lt;/code&gt; will (re)-start from the beginning.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;auto.offset.reset=earliest
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Consumer offsets get committed by default to a Kafka topics. That is a reasonnable default, though sometimes you might prefer to handle them yourself. For example this &lt;a href="http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/"&gt;blog post by Guru Medasani shows how to commit offsets with the processed data in Hbase&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;enable.auto.commit=true
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Group robustness and record delivery guarantees&lt;/h4&gt;
&lt;p&gt;A consumer instance needs to be considered alive to remain part of a consumer group. If it fails to emit hearbeats for more than &lt;code&gt;session.timeout.ms&lt;/code&gt;, it gets kicked out and a group rebalance happens. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;heartbeat.interval.ms=3000

session.timeout.ms=10000
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Kafka consumer refreshes its knowledge of the metadata describing a topic at fixed interval, as defined below. Little known fact: if your consomer start consuming from a topic &lt;em&gt;before you create it&lt;/em&gt; (it's not going to consumme much, is it?), maybe because some consuming client got deployed a bit too early, it will block, that retry discovering the location of the relevant partition after that period as well. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;metadata.max.age.ms=300000
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Kafka 0.11 introduced so-called &lt;em&gt;transactions&lt;/em&gt;. Essentially, they try to mimic the isolation feature of ACID transactions by allowing a producer to mark a set of written records, typically accross several topics, as part of the same atomic write operation. Kafka consumer will ignore that feature by default, unless they are configured with &lt;code&gt;isolation.level=read.committed&lt;/code&gt;, in which case any record that is not part of a committed transaction gets discarded. &lt;/p&gt;
&lt;p&gt;Note that this does &lt;strong&gt;not&lt;/strong&gt; achieves atomic read: this is an all-or-nothing &lt;em&gt;write&lt;/em&gt; operation: from the read side, there is no way to have an all-or-nothing mechanism. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;isolation.level=read_uncommitted
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Sources:&lt;/h1&gt;
&lt;p&gt;Most of the content above has been heavily inspired from the book chapters and blog posts below.&lt;/p&gt;
&lt;p&gt;Kafka, the definitive guide - Gwen Shapira, Todd Palino,  Neha Narkhede:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch02.html#installing_kafka"&gt;chapter 2: installing Kafka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch03.html#writing_messages_to_kafka"&gt;chapter 3: Kafka producers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch04.html"&gt;chapter 4: Kafka consumers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch06.html#reliable_data_delivery"&gt;chapter 6: Reliable data delivery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.udemy.com/apache-kafka-series-setup-administration-in-production/"&gt;Apache Kafka setup series, Kafka setup and administration (Udemy class) - Stephane Maarek&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Confluent blog: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it"&gt;Exactly-once Semantics are Possible: Hereâ€™s How Kafka Does it - Neha Narkhede&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.confluent.io/blog/transactions-apache-kafka/"&gt;Transactions in Apache Kafka - Apurva Mehta, Jason Gustafson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://kafka.apache.org/documentation/#configuration"&gt;Kafka configuration documentation on kafka.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html"&gt;Kafka producer Javadoc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_ha.html"&gt;Configuring High Availability and Consistency for Kafka | 3.0.x | Cloudera Documentation&lt;/a&gt;&lt;/p&gt;</content><category term="kafka"></category></entry><entry><title>Event time low-latency joins with Kafka Streams</title><link href="https://sv3nd.github.io/event-time-low-latency-joins-with-kafka-streams.html" rel="alternate"></link><published>2017-09-17T00:00:00+02:00</published><updated>2017-09-17T00:00:00+02:00</updated><author><name>Svend Vanderveken</name></author><id>tag:sv3nd.github.io,2017-09-17:/event-time-low-latency-joins-with-kafka-streams.html</id><summary type="html">&lt;p&gt;This post attempts to illustrate the difficulty of performing an event-time join between two time series with a stream processing framework. It also describes one solution based on Kafka Streams 0.11.0.0.&lt;/p&gt;
&lt;p&gt;An event-time join is an attempt to join two time series while taking into account the â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post attempts to illustrate the difficulty of performing an event-time join between two time series with a stream processing framework. It also describes one solution based on Kafka Streams 0.11.0.0.&lt;/p&gt;
&lt;p&gt;An event-time join is an attempt to join two time series while taking into account the timestamps. More precisely, for each event from the first time series, it looks up the latest event from the other that occurred before it. This blog post is based on Kafka Stream although I found the original idea in this &lt;a href="http://training.data-artisans.com/exercises/eventTimeJoin.html"&gt;Flink tutorial&lt;/a&gt;, where the idea of event-time join is very well explained. &lt;/p&gt;
&lt;p&gt;Event-time join are often required in practise. For example, given a stream of transactions and another one of customer profile updates, we might want to associate each transaction to the corresponding customer profile as it was known at the moment of the transaction. Or given a stream of traffic information and another one of weather updates, we might want to associate each traffic event with the latest weather that was known for that location at that point in time. &lt;/p&gt;
&lt;p&gt;Note that an event-time join is not symmetric: performing an event-time join from stream 1 to stream 2 does not yield the same result as performing it from stream 2 to stream 1.&lt;/p&gt;
&lt;h1&gt;Difficulty and opportunity related to streaming approach&lt;/h1&gt;
&lt;p&gt;If we were in a batch context, implementing an event-time join would be pretty straightforward. By batch context I mean one where "all the data is available", so that the execution of an aggregation like &lt;code&gt;max(orderDate)&lt;/code&gt; is guaranteed to provide the last order date of the full dataset. &lt;/p&gt;
&lt;p&gt;For example, assume we have a dataset of customer visit events and another one of orders. Both are timestamped and both are thus representing time series. Suppose we want to look up, for each customer visit, the latest order performed by that customer &lt;em&gt;before&lt;/em&gt; the visit. In Batch mode, we can simply look up the latest known order before the visit (&lt;code&gt;Orders.OrderDate &amp;lt;= CustomersVisit.VisitDate&lt;/code&gt; in the example below) and join that to the visit information. One possible illustration might be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt;  One batch approach to linking each customer visit to their latest order that occured before that.&lt;/span&gt;
&lt;span class="cm"&gt;  (probably not optimal, though hopefully clear enough to illustrates my purpose)&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;CustomerOrderAsOfVisitDate&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;VisitId&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;visitID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
       &lt;span class="n"&gt;CustomerOrderAsOfVisitDate&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CustomerId&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;customerId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
       &lt;span class="n"&gt;Orders&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderDate&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;lastOrderDateBeforeVisit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
       &lt;span class="n"&gt;Orders&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ShipperId&lt;/span&gt; &lt;span class="n"&gt;orderShipperId&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;Orders&lt;/span&gt;
    &lt;span class="k"&gt;LEFT&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="c1"&gt;-- latest order date occuring before each visit&lt;/span&gt;
      &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;CustomersVisit&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;VisitId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CustomersVisit&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CustomerId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;orderDate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;lastOrderDate&lt;/span&gt;
          &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;CustomersVisit&lt;/span&gt; 
          &lt;span class="k"&gt;JOIN&lt;/span&gt; &lt;span class="n"&gt;Orders&lt;/span&gt; 
          &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Orders&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CustomerId&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;CustomersVisit&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CustomerId&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt;
              &lt;span class="n"&gt;Orders&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderDate&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;CustomersVisit&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;VisitDate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
          &lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;CustomersVisit&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;VisitId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CustomersVisit&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CustomerId&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;CustomerOrderAsOfVisitDate&lt;/span&gt;
    &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CustomerOrderAsOfVisitDate&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CustomerId&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;Orders&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CustomerId&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt;
        &lt;span class="n"&gt;CustomerOrderAsOfVisitDate&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lastOrderDate&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;Orders&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderDate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;visitID&lt;/th&gt;
&lt;th&gt;customerId&lt;/th&gt;
&lt;th&gt;lastOrderDateBeforeVisit&lt;/th&gt;
&lt;th&gt;orderShipperId&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;Ana Trujillo&lt;/td&gt;
&lt;td&gt;1996-09-18&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;Antonio Moreno&lt;/td&gt;
&lt;td&gt;1996-11-27&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;Around&lt;/td&gt;
&lt;td&gt;1996-12-16&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;Berglunds&lt;/td&gt;
&lt;td&gt;1996-12-16&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A typical crux of stream processing though is the fact that datasets are unbounded and considered theoretically infinite. This implies that at any point in time, we cannot be sure that we have received all necessary information to compute the final version of anything. In the example above, this means that &lt;code&gt;max(orderDate)&lt;/code&gt; only returns the latest order date &lt;em&gt;observed so far&lt;/em&gt;, though that's an aggregation that's ever changing. &lt;/p&gt;
&lt;p&gt;Also, because of delays that could happen during data ingestion, it is typically considered that events are not guaranteed to be delivered in order (see discussion in &lt;a href="https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/event_time.html"&gt;Flink's documentation on Event time vs Processing time&lt;/a&gt; and &lt;a href="https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#handling-event-time-and-late-data"&gt;Spark's time handling documentation&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;This limitation applies also in the case of event-time join: any time we receive a transaction or a car traffic information, we cannot in general be sure that the information we current have concerning user profiles or weather time series is the latest that we will ever be available. We could decide to wait, though how long?&lt;/p&gt;
&lt;p&gt;This question of "how long to wait" is one key difference between stream and batch processing. In a batch approach, some data collection process is assumed to have "waited long enough" beforehand so that at the moment of the batch execution, we can consider that "all data is available". Said otherwise, "waiting long enough" is not a concern of the batch implementation whereas it is a first class citizen in stream processing. &lt;/p&gt;
&lt;p&gt;In many cases though, a nightly batch that processes the last day's data are nothing less than a manual implementation of a 24h &lt;a href="https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/windows.html#tumbling-windows"&gt;tumbling window&lt;/a&gt;. Hiding the stream nature of a dataset behind nightly batches is sometimes hiding too much the complexity related to time by pretending that "all data is available". In many situations, we end up handling ourselves cases like late event arrivals or aggregations over more than one day (e.g. 30 sliding trends), which are much more natural if we use a framework that embrace the infinite time series nature of the dataset. &lt;/p&gt;
&lt;h1&gt;Why not relying on Kafka Streams event-time based processing&lt;/h1&gt;
&lt;p&gt;Kafka Streams 0.11.0.0 does not offer out-of-the box event time join.&lt;/p&gt;
&lt;p&gt;It does provide however a couple of handy primitives for designing stream processing based on event time, as explained in the &lt;a href="https://docs.confluent.io/current/streams/concepts.html#time"&gt;Kafka Streams concepts documentation&lt;/a&gt;. As far as I understand however, these features are primarily useful for &lt;a href="https://docs.confluent.io/current/streams/developer-guide.html#streams-developer-guide-dsl-windowing"&gt;time-based window aggregations&lt;/a&gt; and &lt;a href="http://docs.confluent.io/current/streams/architecture.html?highlight=flow%20control#flow-control-with-timestamps"&gt;best effort flow control&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;Kafka Streams DSL also exposes &lt;a href="https://docs.confluent.io/current/streams/developer-guide.html#kstream-ktable-join"&gt;KStreams-to-KTable join&lt;/a&gt; which essentially corresponds to looking up up-to-date reference data in real time. Confluent has published two excellent blogs about it (&lt;a href="https://www.confluent.io/blog/watermarks-tables-event-time-dataflow-model/"&gt;here&lt;/a&gt; and &lt;a href="https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/"&gt;here&lt;/a&gt;). Combined with Kafka Streams's built-in best-effort flow control, this is already quite powerful and probably exactly what we need in many cases. As a point of comparaison, at the time of writing this, this feature is not (yet?) available as part of Spark Structured Streaming (2.2.0) out of the box.&lt;/p&gt;
&lt;p&gt;Ktable-to-KStream however corresponds to a lookup done at processing time (as mentioned in &lt;a href="https://kafka.apache.org/0110/javadoc/org/apache/kafka/streams/kstream/KStream.html#join(org.apache.kafka.streams.kstream.KTable, org.apache.kafka.streams.kstream.ValueJoiner)"&gt;KStream::join javadoc&lt;/a&gt;). To fully support event-time-join of out of order streams, we need to manually keep some buffers of both streams, which is explained below.&lt;/p&gt;
&lt;h1&gt;High level approach&lt;/h1&gt;
&lt;p&gt;As mentioned in the introduction, this post is inspired from the &lt;a href="http://training.data-artisans.com/exercises/eventTimeJoin.html"&gt;Flink event-time join tutorial&lt;/a&gt;, and my  solution is almost a copy-cat of &lt;a href="https://github.com/dataArtisans/flink-training-exercises/blob/master/src/main/scala/com/dataartisans/flinktraining/exercises/datastream_scala/lowlatencyjoin/EventTimeJoinHelper.scala"&gt;their suggested solution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The gist of my solution is very simple: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;keep in mind that an event-time join is an asymmetric operation. Let's name the first stream the &lt;em&gt;transaction stream&lt;/em&gt; and the one we are join it to the &lt;em&gt;dimension stream&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;upon receiving a &lt;em&gt;dimension event&lt;/em&gt;, just record it in a time-bounded buffer (e.g. TTL = 1 day or so)&lt;/li&gt;
&lt;li&gt;upon receiving a &lt;em&gt;transaction event&lt;/em&gt;, perform a best effort join, i.e. join it with the dimension information available at that moment&lt;/li&gt;
&lt;li&gt;schedule an action that review previously joined information and emits corrected joins when necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an illustration where the transaction stream is a stream of recommendations and the dimension stream is a stream of mood events (this use case is detailed in the code sample below): &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="images/04event-time-join-with-kafka-streams/event-time-join.png"  /&gt; &lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Note that the result of this is what I call a &lt;em&gt;revision log&lt;/em&gt;, i.e. a stream that contains one or several successive revisions of a given information (here, the result of the join). From the point of view of a consumer, only the latest version should be considered for each key. This matches exactly what Kafka Streams calls a &lt;a href="https://docs.confluent.io/current/streams/concepts.html#streams-concepts-ktable"&gt;KTable&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;The code!&lt;/h1&gt;
&lt;h2&gt;Major Kafka Streams components I am using&lt;/h2&gt;
&lt;p&gt;Note that this is all written with Kafka 0.11.0.0 in mind whose API is likely to change in the future. In particular, &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-138%3A+Change+punctuate+semantics"&gt;KIP-138&lt;/a&gt; which is &lt;a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=71764913"&gt;planned to be released as part of Kafka 1.0&lt;/a&gt; will change slightly the semantics of &lt;code&gt;punctuate&lt;/code&gt;, which I rely upon below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;a href="https://docs.confluent.io/current/streams/developer-guide.html#processor-api"&gt;processor API&lt;/a&gt; that allows a low level access to the events. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ability to create a stateful stream by creating a &lt;a href="https://docs.confluent.io/current/streams/developer-guide.html#state-stores"&gt;local though fault-tolerant state stores&lt;/a&gt;. These state stores are local, persistent (backed by a Kafka topic) and transparently re-created on any node in case of restart. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In particular, &lt;a href="https://docs.confluent.io/current/streams/developer-guide.html#fault-tolerant-state-stores"&gt;window store&lt;/a&gt; are an awesome feature that greatly simplifies stream processing since they automatically clean up old data from the local buffer. Also, They allow us to store value associated to a key and a timestamp, like this: &lt;code&gt;store.put(key, value, timestamp)&lt;/code&gt; and then retrieve all values for this key within a given time range, like this:  &lt;code&gt;store.fetch(key, fromTime, toTime)&lt;/code&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The API allow us to &lt;a href="https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/KStreamBuilder.html#stream-org.apache.kafka.common.serialization.Serde-org.apache.kafka.common.serialization.Serde-java.lang.String...-"&gt;listen to several topics at once&lt;/a&gt; and even to listen to a &lt;a href="https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/KStreamBuilder.html#stream-java.util.regex.Pattern-"&gt;topic name regexp&lt;/a&gt; ! This simple feature removes the need in this case of specific API for multiple inputs like &lt;a href="https://ci.apache.org/projects/flink/flink-docs-release-1.3/api/java/org/apache/flink/streaming/api/functions/co/CoProcessFunction.html"&gt;Flink's CoProcessFunction&lt;/a&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Code walkthrough&lt;/h2&gt;
&lt;p&gt;With all this explained and explored, the actual implementation is pretty straightforward. I pushed a proof of concept in Scala to my &lt;a href="https://github.com/sv3nd/event-time-join-with-kafka-streams"&gt;event-time-join-with-kafka-streams github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this example, we are going to process an event stream of consultant's recommendation (I'm a consultant myself, so I know best). A recommendation event looks like that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;ingestion_time&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;697&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;consultant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Sheila&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;event_time&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;520&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;recommendation&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;seize proactive interfaces&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to perform &lt;em&gt;advanced&lt;/em&gt; analytics, we're going to join it with stream of known mood of said consultants. Here's an example of consultant's mood event: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;ingestion_time&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;696&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Tammy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;event_time&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;mood&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;neutral&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Each of these events are delivered potentially out of order with arbitrary delays. We want to perform an event-time join, i.e. for each recommendation event, lookup the most recent known mood before the recommendation.&lt;/p&gt;
&lt;p&gt;The processing topology is very simple, we just parse the incoming &lt;code&gt;mood&lt;/code&gt; or &lt;code&gt;recommendation&lt;/code&gt; events and provide them to our &lt;code&gt;EventTimeJoiner&lt;/code&gt; implementation: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;parsed&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;builder&lt;/span&gt;
  &lt;span class="c1"&gt;// read events from both Kafka topics&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;Serdes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;String&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="nc"&gt;Serdes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;String&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;etj-moods&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;etj-events&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;// parse the Json into a stream of Either[Recommendation, Mood]&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapValues&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Option&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Either&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Recommendation&lt;/span&gt;, &lt;span class="kt"&gt;Mood&lt;/span&gt;&lt;span class="o"&gt;]]](&lt;/span&gt;&lt;span class="nc"&gt;EventTimeJoinExample&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Option&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Either&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Recommendation&lt;/span&gt;, &lt;span class="kt"&gt;Mood&lt;/span&gt;&lt;span class="o"&gt;]])&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt;  &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isDefined&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapValues&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Either&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Recommendation&lt;/span&gt;, &lt;span class="kt"&gt;Mood&lt;/span&gt;&lt;span class="o"&gt;]](&lt;/span&gt;&lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;// watch out: selectKey does not trigger repartitioning. My test data is already partionned =&amp;gt; in this specific case it&amp;#39;s ok&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;selectKey&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;]{&lt;/span&gt; &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nc"&gt;EventTimeJoinExample&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;userId&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;)}&lt;/span&gt;

  &lt;span class="c1"&gt;// actual event-time join&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;EventTimeJoiner&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;supplier&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;moods&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;bestEffortJoins&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="s"&gt;&amp;quot;consultants&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;EventTimeJoiner&lt;/code&gt; maintains 3 State stores: one containing the time series of moods of each consultant, another containing the joined events we have emitted recently and finally a 3rd one to recall all the consultant's names we have encountered recently: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;moodStore&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;KeyValueStore&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;, &lt;span class="kt"&gt;List&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Mood&lt;/span&gt;&lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;bestEffortJoinsStore&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;WindowStore&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;, &lt;span class="kt"&gt;MoodRec&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;consultantNamesStore&lt;/span&gt; &lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;WindowStore&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;, &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the &lt;code&gt;transform()&lt;/code&gt; method we have most of the high level logic mentioned above: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if we receive a mood event, we just record that mood within the time series of the corresponding consultant in the &lt;code&gt;moodStore&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;if we receive a recommendation event, we join it to the mood information we currently have, we record that in the &lt;code&gt;bestEffortjoinStore&lt;/code&gt; and we emit it &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;event&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Either&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Recommendation&lt;/span&gt;, &lt;span class="kt"&gt;Mood&lt;/span&gt;&lt;span class="o"&gt;])&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;KeyValue&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;, &lt;span class="kt"&gt;MoodRec&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt;
  &lt;span class="n"&gt;event&lt;/span&gt; &lt;span class="k"&gt;match&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nc"&gt;Left&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt;
      &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;joined&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;event_time&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;consultant&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;recommendation&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;bestEffortJoinsStore&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;consultant&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;joined&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;event_time&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;recordConsultantName&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;consultant&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;event_time&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;KeyValue&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;joined&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nc"&gt;Right&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mood&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt;
      &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;updatedMoodHistory&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mood&lt;/span&gt; &lt;span class="o"&gt;::&lt;/span&gt; &lt;span class="n"&gt;moodHistory&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mood&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)).&lt;/span&gt;&lt;span class="n"&gt;sortBy&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;event_time&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;moodStore&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mood&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;updatedMoodHistory&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;recordConsultantName&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mood&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mood&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;event_time&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="kc"&gt;null&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;recommendationTime&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Long&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consultant&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recommendation&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;MoodRec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
 &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;maybeMood&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;moodHistory&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;consultant&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropWhile&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;event_time&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;recommendationTime&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;headOption&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mood&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  &lt;span class="nc"&gt;MoodRec&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;recommendationTime&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consultant&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maybeMood&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recommendation&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;    
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is only half of the story of course, since we also need to schedule a periodic review of that join result. This is very easy do to in Kafka Streams by simply requesting our &lt;code&gt;punctuate()&lt;/code&gt; method to be invoked every (say) 1000ms &lt;em&gt;in event time&lt;/em&gt; (whose definition depends on how we configured our &lt;a href="https://docs.confluent.io/current/streams/developer-guide.html#streams-developer-guide-timestamp-extractor"&gt;timestamp-extractor&lt;/a&gt;, and here again keep in mind that &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-138%3A+Change+punctuate+semantics"&gt;KIP-138&lt;/a&gt; is on its way to revisit that): &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;ProcessorContext&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Unit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;    
  &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schedule&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And once we're there we're essentially done. All that's left to do is to let &lt;code&gt;punctuate()&lt;/code&gt; revisit the join result of all recently witnessed consultants and, if any difference is found, re-emit the updated join result. Note in passing the awesome &lt;code&gt;fetch()&lt;/code&gt; method of the window store which let us easily query a time series by range. The &lt;code&gt;joinAgain()&lt;/code&gt; method is not shown here, though essentially it just revisit the join result&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;punctuate&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latestEventTime&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Long&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;KeyValue&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;, &lt;span class="kt"&gt;MoodRec&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;allRecentConsultants&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;until&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;latestEventTime&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;foreach&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;joinAgain&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxEventTimestamp&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;latestEventTime&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;reviewLagDelta&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="kc"&gt;null&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;allRecentConsultants&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;until&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Long&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Iterator&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt;
  &lt;span class="n"&gt;consultantNamesStore&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fetch&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;all-recent-names&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nc"&gt;BEGINNING_OF_TIMES&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;until&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;asScala&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that my code at that point gets a bit dirty and in particular, lacks some necessary clean up of some key-value store. Yeah, I'm that lazy...&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I really love the Kafka Streams abstractions, the duality between logs and tables is beautifully present a bit everywhere, which offers a refreshing way of designing both data processing solutions but also plain reactive micro-services. Also, Kafka Streams is just a very thin layer on top of the core functionalities offered by Kafka clusters and Kafka consumers/producers. A KTable is essentially a compacted topics on steroids, a join boils down to pushing key-values to a topic and letting Kafka's partitioning by key co-locating the data, local distributed state stores are event-sourced to Kafka topics again which makes them transparently replicated....&lt;/p&gt;
&lt;p&gt;This of course means that Kafka streams is very tightly coupled to the underlying Kafka cluster. This is a very different positioning than Flink or Spark Structured Streaming which are planned for various streamed input and output technologies.&lt;/p&gt;
&lt;p&gt;I like less the current java-only API which is very OO-based, encourages mutating objects and makes explicit use of null values. For example instances of &lt;code&gt;Transformer&lt;/code&gt; (and several other API components) must be classes created with null instance members that are later on initialised in the &lt;code&gt;init()&lt;/code&gt;. My preference would have been for relying more on immutable constructs. Likewise differences between scala and java generics semantics imply that &lt;a href="https://docs.confluent.io/current/streams/faq.html#scala-compile-error-no-type-parameter-java-defined-trait-is-invariant-in-type-t"&gt;scala code becomes less elegant than what it could be&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Kafka Streams is a young project though, so one can hope that a functional-oriented and scala based API might be offered at some point in the future (pretty please?)&lt;/p&gt;</content><category term="kafka"></category><category term="kafka-streams"></category><category term="stream-processing"></category><category term="scala"></category></entry><entry><title>How to integrate Flink with Confluent's schema registry</title><link href="https://sv3nd.github.io/how-to-integrate-flink-with-confluents-schema-registry.html" rel="alternate"></link><published>2017-06-30T00:00:00+02:00</published><updated>2017-06-30T00:00:00+02:00</updated><author><name>Svend Vanderveken</name></author><id>tag:sv3nd.github.io,2017-06-30:/how-to-integrate-flink-with-confluents-schema-registry.html</id><summary type="html">&lt;p&gt;This post illustrates how to use Confluent's Avro serializer in order to let a Flink program consume and produce avro messages through Kafka while keeping track of the Avro Schemas in Confluent's schema registry. This can be interresting if the messages are pumped into or out of Kafka with Kafka â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post illustrates how to use Confluent's Avro serializer in order to let a Flink program consume and produce avro messages through Kafka while keeping track of the Avro Schemas in Confluent's schema registry. This can be interresting if the messages are pumped into or out of Kafka with Kafka Connect, Kafka Streams, or just with anything else also integrated with the schema registry.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: As of now (Aug 2017), it turns out using Confluent's Avro deserializer as explained below is not ideal when deploying to FLink in standalone mode, because of the way caching is impemented on Avro level. More information in &lt;a href="https://github.com/confluentinc/schema-registry/pull/509#issuecomment-323143951"&gt;this Confluent PR&lt;/a&gt; as well as in &lt;a href="https://issues.apache.org/jira/browse/FLINK-5633"&gt;this FLink JIRA&lt;/a&gt;. Hopefully a workaround will be found soon.&lt;/p&gt;
&lt;p&gt;This has been written with the following dependencies in mind: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;libraryDependencies&lt;/span&gt; &lt;span class="o"&gt;++=&lt;/span&gt; &lt;span class="nc"&gt;Seq&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
  &lt;span class="s"&gt;&amp;quot;org.apache.flink&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;flink-scala&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;1.3.1&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;provided&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;&amp;quot;org.apache.flink&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;flink-streaming-scala&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;1.3.1&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;provided&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;&amp;quot;org.apache.flink&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;flink-connector-kafka-0.10&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;1.3.1&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;

  &lt;span class="s"&gt;&amp;quot;io.confluent&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;kafka-avro-serializer&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;3.2.2&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Confluent's schema registry library&lt;/h2&gt;
&lt;p&gt;Confluent has published their version of an &lt;a href="https://github.com/confluentinc/schema-registry/tree/3.2.1-post/avro-serializer"&gt;Avro Serializer&lt;/a&gt; which automatically (and idempotently) registers the Avro schema into the schema registry when performing serialization (as &lt;a href="https://github.com/confluentinc/schema-registry/blob/3.2.1-post/avro-serializer/src/main/java/io/confluent/kafka/serializers/AbstractKafkaAvroSerializer.java#L72"&gt;visible here&lt;/a&gt;). The convention they use is simply to declare 2 &lt;em&gt;subjects&lt;/em&gt; within the registry for each kafka topic, called &lt;em&gt;&amp;lt;topic-name&gt;-value&lt;/em&gt; and &lt;em&gt;&amp;lt;topic-name&gt;-key&lt;/em&gt; and put the schema there. This allows the de-serializer to &lt;a href="https://github.com/confluentinc/schema-registry/blob/3.2.1-post/avro-serializer/src/main/java/io/confluent/kafka/serializers/AbstractKafkaAvroDeserializer.java#L121"&gt;retrieve the schema when needed&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Flink Kafka consumer&lt;/h2&gt;
&lt;p&gt;There are various aspects to tackle when adding a Kafka consumer as a stream source to Flink. The one we're focusing on here is &lt;a href="https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/kafka.html#the-deserializationschema"&gt;the deserializations schema&lt;/a&gt;. This class is the place where we can specify to Flink how handle the &lt;code&gt;byte[]&lt;/code&gt; consumed from Kafka, so all we have to do is to plug there Confluent's schema-registry aware Avro deserializer. &lt;/p&gt;
&lt;p&gt;It goes like this: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;io.confluent.kafka.serializers.&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="nc"&gt;AbstractKafkaAvroSerDeConfig&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nc"&gt;KafkaAvroDeserializer&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.avro.generic.GenericRecord&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.flink.api.common.typeinfo.TypeInformation&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.flink.api.java.typeutils.TypeExtractor&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema&lt;/span&gt;

&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;KafkaKV&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ConfluentRegistryDeserialization&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;schemaRegistryUrl&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; 
      &lt;span class="k"&gt;extends&lt;/span&gt; &lt;span class="nc"&gt;KeyedDeserializationSchema&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;KafkaKV&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;

  &lt;span class="c1"&gt;// Flink needs the serializer to be serializable =&amp;gt; this &amp;quot;@transient lazy val&amp;quot; does the trick&lt;/span&gt;
  &lt;span class="nd"&gt;@transient&lt;/span&gt; &lt;span class="k"&gt;lazy&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;valueDeserializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;deserializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;KafkaAvroDeserializer&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;deserializer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;configure&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
      &lt;span class="c1"&gt;// other schema-registry configuration parameters can be passed, see the configure() code &lt;/span&gt;
      &lt;span class="c1"&gt;// for details (among other things, schema cache size)&lt;/span&gt;
      &lt;span class="nc"&gt;Map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;AbstractKafkaAvroSerDeConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;SCHEMA_REGISTRY_URL_CONFIG&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;schemaRegistryUrl&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;asJava&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; 
      &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;deserializer&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

  &lt;span class="nd"&gt;@transient&lt;/span&gt; &lt;span class="k"&gt;lazy&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;keyDeserializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;deserializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;KafkaAvroDeserializer&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;deserializer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;configure&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
      &lt;span class="nc"&gt;Map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;AbstractKafkaAvroSerDeConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;SCHEMA_REGISTRY_URL_CONFIG&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;schemaRegistryUrl&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;asJava&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; 
      &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;deserializer&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

  &lt;span class="k"&gt;override&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;isEndOfStream&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nextElement&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;KafkaKV&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Boolean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;

  &lt;span class="k"&gt;override&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;deserialize&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;messageKey&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Byte&lt;/span&gt;&lt;span class="o"&gt;],&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Byte&lt;/span&gt;&lt;span class="o"&gt;],&lt;/span&gt; 
                           &lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Int&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;offset&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Long&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;KafkaKV&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;

    &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;keyDeserializer&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;messageKey&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;asInstanceOf&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;valueDeserializer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deserialize&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;asInstanceOf&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

    &lt;span class="nc"&gt;KafkaKV&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

  &lt;span class="k"&gt;override&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;getProducedType&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;TypeInformation&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;KafkaKV&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; 
      &lt;span class="nc"&gt;TypeExtractor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getForClass&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classOf&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;KafkaKV&lt;/span&gt;&lt;span class="o"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once this is in place, we can use it to create a Flink Kafka source as follows: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.flink.api.scala._&lt;/span&gt;
  &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;/span&gt;
  &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010&lt;/span&gt;

  &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;...&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="nc"&gt;StreamExecutionEnvironment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getExecutionEnvironment&lt;/span&gt;

  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;kafkaConsumerConfig&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;kafkaStream&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addSource&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
      &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;FlinkKafkaConsumer010&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;KafkaKV&lt;/span&gt;&lt;span class="o"&gt;](&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;someInboundTopic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;ConfluentRegistryDeserialization&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;someInboundTopic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;http://localhost:8081&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;kafkaConsumerConfig&lt;/span&gt;
        &lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Flink Kafka producer&lt;/h2&gt;
&lt;p&gt;This is exactly the same story: in order to be able to produce avro messages into Kafka with Flink while automatically registering their schema in the registry, all we have to do is provide a Flink serializer that is essentially an adapter to Confluent's Avro serializer. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="kt"&gt;KafkaKey&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nc"&gt;String&lt;/span&gt;
&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SomePojo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ConfluentRegistrySerialization&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;schemaRegistryUrl&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; 
        &lt;span class="k"&gt;extends&lt;/span&gt; &lt;span class="nc"&gt;KeyedSerializationSchema&lt;/span&gt;&lt;span class="o"&gt;[(&lt;/span&gt;&lt;span class="kt"&gt;KafkaKey&lt;/span&gt;, &lt;span class="kt"&gt;SomePojo&lt;/span&gt;&lt;span class="o"&gt;)]{&lt;/span&gt;

  &lt;span class="nd"&gt;@transient&lt;/span&gt; &lt;span class="k"&gt;lazy&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;valueSerializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;serializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;KafkaAvroSerializer&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;serializer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;configure&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
      &lt;span class="nc"&gt;Map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;AbstractKafkaAvroSerDeConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;SCHEMA_REGISTRY_URL_CONFIG&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;schemaRegistryUrl&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;asJava&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
      &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;serializer&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

  &lt;span class="nd"&gt;@transient&lt;/span&gt; &lt;span class="k"&gt;lazy&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;keySerializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;serializer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;KafkaAvroSerializer&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;serializer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;configure&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
      &lt;span class="nc"&gt;Map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;AbstractKafkaAvroSerDeConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;SCHEMA_REGISTRY_URL_CONFIG&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;schemaRegistryUrl&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;asJava&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
      &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;serializer&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

  &lt;span class="k"&gt;override&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;serializeKey&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;keyedMessages&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;KafkaKey&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;SomePojo&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Byte&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;keySerializer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;serialize&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keyedMessages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;override&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;getTargetTopic&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;element&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;KafkaKey&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;SomePojo&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;topic&lt;/span&gt;

  &lt;span class="k"&gt;override&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;serializeValue&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;keyedMessages&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;KafkaKey&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;SomePojo&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;Byte&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt;
     &lt;span class="n"&gt;valueSerializer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;serialize&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keyedMessages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And again, once this serialization adapter is there, all we have to do is &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;kafkaProducerConfig&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;someStream&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kafkaStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;blabla&lt;/span&gt;&lt;span class="o"&gt;)...&lt;/span&gt;

  &lt;span class="nc"&gt;FlinkKafkaProducer010&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writeToKafkaWithTimestamps&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;someStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;javaStream&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;destinationTopic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;AvroRegistrySerialization&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;destinationTopic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;http://localhost:8081&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;kafkaProducerConfig&lt;/span&gt;&lt;span class="o"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That's about it :) &lt;/p&gt;</content><category term="flink"></category><category term="avro"></category><category term="kafka"></category></entry><entry><title>Sending Avro records from Scala to Azure eventhub over AMQP</title><link href="https://sv3nd.github.io/sending-avro-records-from-scala-to-azure-eventhub-over-amqp.html" rel="alternate"></link><published>2017-06-26T00:00:00+02:00</published><updated>2017-06-26T00:00:00+02:00</updated><author><name>Svend Vanderveken</name></author><id>tag:sv3nd.github.io,2017-06-26:/sending-avro-records-from-scala-to-azure-eventhub-over-amqp.html</id><summary type="html">&lt;p&gt;This post illustrates how to emit Avro records to Azure EventHub from scala in such a way that they are directly parsed by the other services of the Azure platform (e.g. Azure Stream Analytics). &lt;/p&gt;
&lt;p&gt;There exists a Java API for communicating with Azure EventHub which is documented as part â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post illustrates how to emit Avro records to Azure EventHub from scala in such a way that they are directly parsed by the other services of the Azure platform (e.g. Azure Stream Analytics). &lt;/p&gt;
&lt;p&gt;There exists a Java API for communicating with Azure EventHub which is documented as part of the &lt;a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-java-get-started-send"&gt;azure documentation&lt;/a&gt; and even made &lt;a href="https://github.com/Azure/azure-event-hubs-java"&gt;open source on github&lt;/a&gt; (things have changed at Microsoft...). That said, the most detailed documentation still seems to be based on the .NET API as manipulated with Visual Studio on Windows. Me being a Scala developer on a Mac, it took me a bit of experimentation to emit Avro messages to EventHub and have an Azure Stream Analytics job parse them correctly.&lt;/p&gt;
&lt;p&gt;The steps below assume that you have access to the Azure portal and have created an EventHub namespace as well as an EventHub instance. If not, see the &lt;a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create"&gt;Azure documentation here&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;I posted a fully working example &lt;a href="https://github.com/svendx4f/avro-eventhub-scala-example"&gt;here on github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This post has been written with the following frawmework versions in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avro 1.8.2&lt;/li&gt;
&lt;li&gt;Azure EventHub SDK 0.14&lt;/li&gt;
&lt;li&gt;Scala 2.11&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Scala Avro record emitter&lt;/h2&gt;
&lt;p&gt;In a nutshell, these are the few things to know when sending Avro to Azure EventHub: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each message sent to Azure is wrapped as part of an instance of &lt;a href="https://github.com/Azure/azure-event-hubs-java/blob/dev/azure-eventhubs/src/main/java/com/microsoft/azure/eventhubs/EventData.java"&gt;EventData&lt;/a&gt;, which also contains meta-data regarding the AMQP transmission.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;byte[]&lt;/code&gt; payload that we wrap inside an &lt;code&gt;EventData&lt;/code&gt; instance should be a serialized Avro &lt;em&gt;file&lt;/em&gt;, i.e. contain the Avro schema. This means we should use the Avro &lt;a href="https://github.com/apache/avro/blob/branch-1.8/lang/java/avro/src/main/java/org/apache/avro/file/DataFileWriter.java"&gt;DataFileWriter&lt;/a&gt; and serialize the output directly to an output stream to obtain the &lt;code&gt;byte[]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;As for any Avro file, it is possible, and actually a very good idea, to put several avro records within the &lt;code&gt;EventData&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Based on my experimentations, only uncompressed Avro records or records compressed with &lt;a href="https://en.wikipedia.org/wiki/DEFLATE"&gt;Deflate&lt;/a&gt; seem to be currently supported by Azure. During my tests, neither bzip2 nor Snappy could be read by the Stream Analytics job. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an extract of the scala code (see &lt;a href="https://github.com/svendx4f/avro-eventhub-scala-example/blob/master/src/main/scala/org/svend/playground/EventHubAvroSender.scala"&gt;EventHubAvroSender on github&lt;/a&gt; for details)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;com.microsoft.azure.eventhubs.&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="nc"&gt;EventData&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nc"&gt;EventHubClient&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.avro.file.&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="nc"&gt;Codec&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nc"&gt;CodecFactory&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nc"&gt;DataFileWriter&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.avro.generic.&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="nc"&gt;GenericDatumWriter&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nc"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scala.collection.JavaConverters._&lt;/span&gt;

&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;...&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;ehClient&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="nc"&gt;EventHubClient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createFromConnectionStringSync&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;lt;your eventhub connection string&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;// number of batches to send to the EventHub&lt;/span&gt;
  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;batch_num&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

  &lt;span class="c1"&gt;// number of EventData instances to put inside each batch&lt;/span&gt;
  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;amqpMessagePerBatch&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;

  &lt;span class="c1"&gt;// number of avro records to bundle inside each AMQP message&lt;/span&gt;
  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;userMessagesPerAmqp&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;datumWriter&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;GenericDatumWriter&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;](&lt;/span&gt;&lt;span class="nc"&gt;UserMessage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;writer&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;DataFileWriter&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;](&lt;/span&gt;&lt;span class="n"&gt;datumWriter&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;// only Deflate seems to be compatible with Azure at the moment &lt;/span&gt;
  &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setCodec&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;CodecFactory&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deflateCodec&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;  
  &lt;span class="c1"&gt;//writer.setCodec(CodecFactory.snappyCodec()) // not currently supported&lt;/span&gt;
  &lt;span class="c1"&gt;//writer.setCodec(CodecFactory.bzip2Codec())  // not currently supported&lt;/span&gt;

  &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;batch_num&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="n"&gt;batchid&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt;

      &lt;span class="c1"&gt;// list of EventData instances, each with a bunch of Avro records&lt;/span&gt;
      &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;eventHubMessages&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;amqpMessagePerBatch&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt;

        &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;bos&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;ByteArrayOutputStream&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;UserMessage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bos&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;// MessageGen.genMessage is a generator of random data&lt;/span&gt;
        &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;userMessagesPerAmqp&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;foreach&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;MessageGen&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;genMessage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toAvro&lt;/span&gt;&lt;span class="o"&gt;)}&lt;/span&gt;

        &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;bos&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;EventData&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bos&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toByteArray&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;}&lt;/span&gt;

      &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;sending batch &lt;/span&gt;&lt;span class="si"&gt;$batchid&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

      &lt;span class="c1"&gt;// this sends a batch of EventData asynchronously and returns a Java Future&lt;/span&gt;
      &lt;span class="n"&gt;ehClient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;send&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eventHubMessages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asJava&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

  &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;waiting for all futures before exiting...&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;foreach&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;

  &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;ok, closing&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;ehClient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that there are two batching mechanisms at play above: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the set of &lt;code&gt;userMessagesPerAmqp&lt;/code&gt; avro records we put inside each &lt;code&gt;EventData&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;the set of &lt;code&gt;amqpMessagePerBatch&lt;/code&gt; AMQP messages that are sent as part of one AMQP batch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I did not investigate what was the ideal combination of those two.&lt;/p&gt;
&lt;h2&gt;Stream Analytics Job&lt;/h2&gt;
&lt;p&gt;Azure stream analytics usage is described in &lt;a href="https://docs.microsoft.com/en-us/azure/stream-analytics/"&gt;Azure documentation here&lt;/a&gt;, they essentially let you execute an on-going &lt;a href="https://docs.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-u-sql-get-started"&gt;U-SQL&lt;/a&gt; query on data streaming out of an EventHub instance, IotHub instance or Blob Storage and forward the result to various outputs.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="images/sending-avro-events-to-azure-eventhub-from-scala/input-query-output.png"  width="400px" /&gt; &lt;/center&gt;&lt;/p&gt;
&lt;p&gt;All fields of the Avro schema are available in the query, so based on our example schema: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;namespace&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;svend.playground.user&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;record&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;User&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;fields&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;user_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;int&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;null&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mood&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;string&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;null&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;message&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;string&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;null&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can access each field as part of a query in the U-SQL editor of the Azure Stream Analytics Job: &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="images/sending-avro-events-to-azure-eventhub-from-scala/example_query.png"  width="280px" /&gt; &lt;/center&gt;&lt;/p&gt;
&lt;p&gt;You might notice the presence of &lt;code&gt;EventEnqueuedUtcTime&lt;/code&gt; in the query above, this is one of the supplementary fields that Azure EventHub adds to each received event, as &lt;a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs"&gt;specified in the Azure documentation&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="images/sending-avro-events-to-azure-eventhub-from-scala/supplementary_usql_fields.png"  width="500px" /&gt; &lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Once everything is started, the Stream Analytics page on the Azure portal should start to show some process traffic: &lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="images/sending-avro-events-to-azure-eventhub-from-scala/analytics_traffic.png"  width="500px" /&gt; &lt;/center&gt;&lt;/p&gt;</content><category term="avro"></category><category term="scala"></category><category term="azure"></category><category term="cloud"></category></entry></feed>