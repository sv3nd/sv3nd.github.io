<!DOCTYPE html>
<html lang="en">
	<head>
		<link href="http://gmpg.org/xfn/11" rel="profile">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta http-equiv="content-type" content="text/html; charset=utf-8">

		<!-- Enable responsiveness on mobile devices-->
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

		<title>Svend Vanderveken</title>

		<!-- CSS -->
		<link href="//fonts.googleapis.com/" rel="dns-prefetch">
		<link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Abril+Fatface|PT+Sans:400,400italic,700&amp;subset=latin,latin-ext" rel="stylesheet">

		<link rel="stylesheet" href="https://svend.kelesia.com/theme/css/poole.css" />
		<link rel="stylesheet" href="https://svend.kelesia.com/theme/css/hyde.css" />
		<link rel="stylesheet" href="https://svend.kelesia.com/theme/css/syntax.css" />
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

		<!-- RSS -->
		<link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
	<script type="text/javascript">
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 			})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
			ga('create', 'UA-101598127-1', 'auto');
			ga('send', 'pageview');
	</script>
	</head>

	<body class="theme-base-0d">
<div class="sidebar">
	<div class="container sidebar-sticky">
		<div class="sidebar-about">

			<h1>
				<a href="/">
					<img class="profile-picture" src="https://svend.kelesia.com/images/blog/svend.jpg">
					Svend Vanderveken
				</a>
			</h1>
			<p class="lead"></p>
			<p class="lead">I am a freelance data engineer, I currently focus on streaming architectures, Kafka, Scala, Python, SQL,... </p>
			<p></p>
		</div>
		<nav class="sidebar-nav">
					<a class="sidebar-nav-item" href="https://github.com/sv3ndk">
						<i class="fa fa-github"></i>
					</a>
					<a class="sidebar-nav-item" href="https://twitter.com/sv3ndk">
						<i class="fa fa-twitter"></i>
					</a>
					<a class="sidebar-nav-item" href="https://be.linkedin.com/in/vanderveken">
						<i class="fa fa-linkedin"></i>
					</a>
					<a class="sidebar-nav-item" href="http://stackoverflow.com/users/3318335/svend">
						<i class="fa fa-stack-overflow"></i>
					</a>
			<a class="sidebar-nav-item" href="">
				<i class="fa fa-feed"></i>
			</a>
		</nav>
	</div>
</div>		<div class="content container">
	<div class="posts">
			<div class="post">
				<h1 class="post-title" href="https://svend.kelesia.com/using-model-post-processor-within-scikit-learn-pipelines.html#using-model-post-processor-within-scikit-learn-pipelines">
					<a href="https://svend.kelesia.com/using-model-post-processor-within-scikit-learn-pipelines.html#using-model-post-processor-within-scikit-learn-pipelines">Using model post-processor within scikit-learn pipelines</a>
				</h1>
				<span class="post-date">Tue 18 October 2016</span>
				<p>
					
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="On-the-lack-of-post-processors">On the lack of post-processors<a class="anchor-link" href="#On-the-lack-of-post-processors">¶</a></h2><p><a href="http://scikit-learn.org/stable/modules/pipeline.html#pipeline">Pipelines</a> are a very handy and central feature of the scikit-learn library that enable to chain sequences, or even, in a limited way, DAGs, of data transformers and one machine learning model. They greatly facilitate the application of cross-validation scoring or hyper-parameter search on the resulting pipeline as a whole. They also ease the productization of a machine learning exercise by providing one single encapsulated trained model that can be applied as one instance on a test set or integrated into a production product.</p>
<p>For a very clear introduction, see also the excellent <a href="http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html">Using scikit-learn Pipelines and FeatureUnions</a>
				</p>
				<a class="read-more" href="using-model-post-processor-within-scikit-learn-pipelines.html">Continue reading »</a>
			</div>
			<div class="post">
				<h1 class="post-title" href="https://svend.kelesia.com/how-to-normalize-log-likelihood-vectors.html#how-to-normalize-log-likelihood-vectors">
					<a href="https://svend.kelesia.com/how-to-normalize-log-likelihood-vectors.html#how-to-normalize-log-likelihood-vectors">How to normalize log-likelihood vectors</a>
				</h1>
				<span class="post-date">Sun 02 October 2016</span>
				<p>
					
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is the best way to convert a log-likelihood vector into the corresponding log-probability vector, i.e. a vector of values whose exponential sum up to one?</p>
<p>TLDR: just subtract the <code>scipy.misc.logsumexp()</code> of that vector.</p>
<p>Note: this notebook is <a href="https://github.com/svendx4f/svendx4f.github.io/blob/dev/content/how_to_normalize_likelihood_vectors.ipynb">available on Github</a> in case you want to play around with it.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Usage-of-logs-in-probability-and-likelihood-computations">Usage of logs in probability and likelihood computations<a class="anchor-link" href="#Usage-of-logs-in-probability-and-likelihood-computations">¶</a></h2><p>Given a set of independent events $D= \{E_i\}$ with probabilities $\{p_i\}$, the probability of the joint occurance of all events in $D$ is simply</p>
$$P_D = \prod\limits_i^N p_i$$<p>This quantity however quickly gets impossible to compute on a computer as-written above since, each $p_i$ $\in [0,1]$, the result gets smaller and smaller as the multiplication goes on and quickly leads to underflow. Usage of logarithms of probabilities and of likelihoods is a very common computational trick to avoid that underflow which exploits the fact that a logarithm transforms a product into a sum, i.e.</p>
				</p>
				<a class="read-more" href="how-to-normalize-log-likelihood-vectors.html">Continue reading »</a>
			</div>
	</div>
	<div class="pagination">

		<span class="pagination-item older">Newer</span>

		<span class="pagination-item newer">Older</span>
	</div>
		</div>
	</body>
</html>