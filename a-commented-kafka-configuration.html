<!DOCTYPE html>
<html lang="en">
	<head>
		<link href="http://gmpg.org/xfn/11" rel="profile">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta http-equiv="content-type" content="text/html; charset=utf-8">

		<!-- Enable responsiveness on mobile devices-->
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

		<title>Svend Vanderveken</title>

		<!-- CSS -->
		<link href="//fonts.googleapis.com/" rel="dns-prefetch">
		<link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Abril+Fatface|PT+Sans:400,400italic,700&amp;subset=latin,latin-ext" rel="stylesheet">

		<link rel="stylesheet" href="https://svend.kelesia.com/theme/css/poole.css" />
		<link rel="stylesheet" href="https://svend.kelesia.com/theme/css/hyde.css" />
		<link rel="stylesheet" href="https://svend.kelesia.com/theme/css/syntax.css" />
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

		<!-- RSS -->
		<link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
	<script type="text/javascript">
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 			})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
			ga('create', 'UA-101598127-1', 'auto');
			ga('send', 'pageview');
	</script>
	</head>

	<body class="theme-base-0d">
<div class="sidebar">
	<div class="container sidebar-sticky">
		<div class="sidebar-about">

			<h1>
				<a href="/">
					<img class="profile-picture" src="https://svend.kelesia.com/images/blog/svend-profile.jpg">
					Svend Vanderveken
				</a>
			</h1>
			<p class="lead"></p>
			<p class="lead">I am a freelance data engineer, I currently focus on streaming architectures, Kafka, Scala, Python, SQL,... </p>
			<p></p>
		</div>
		<nav class="sidebar-nav">
					<a class="sidebar-nav-item" href="https://github.com/sv3ndk">
						<i class="fa fa-github"></i>
					</a>
					<a class="sidebar-nav-item" href="https://twitter.com/sv3ndk">
						<i class="fa fa-twitter"></i>
					</a>
					<a class="sidebar-nav-item" href="https://be.linkedin.com/in/vanderveken">
						<i class="fa fa-linkedin"></i>
					</a>
					<a class="sidebar-nav-item" href="https://stackoverflow.com/users/3318335/svend">
						<i class="fa fa-stack-overflow"></i>
					</a>
			<a class="sidebar-nav-item" href="">
				<i class="fa fa-feed"></i>
			</a>
		</nav>
	</div>
</div>		<div class="content container">
<div class="post">
	<h1 class="post-title">A commented Kafka configuration</h1>
	<span class="post-date">Tue 19 December 2017</span>
	<p>Diving into Kafka configuration is a beautiful journey into its features. </p>
<p>As a preparation for a production deployment of Kafka 0.11, I gathered a set of comments on what I think are some interesting parameters. All this amazing wisdom is mostly extracted from the few resources mentioned at the end of this post. </p>
<h3>A grain of salt...</h3>
<p>This is all for information only, I honestly think most of the points below are relevant and correct, though mistakes and omissions are likely present here and there. </p>
<p>You should not apply any of this blindly to your production environment and hope for anything to work. </p>
<p>The wiser thing to do <em>of course</em> is renting my services, I'm freelance, see my contact references beside :)  </p>
<h3>Broker basic parameters</h3>
<p>Let's start with a few comments on Kafka broker basic parameters, maybe located somewhere like <code>etc/kafka/server.properties</code></p>
<p>First off, each broker must know its enpoint as known by consumers and producers. This is because a Kafka cluster keeps a dynamic list of which broker serves which topic partition. Consumers and producers then obtain that routing information as part of the topic metadata and connect directly to the appropriate broker when exchanging data. </p>
<div class="highlight"><pre><span></span>listeners=PLAINTEXT://your.host.name:9092
</pre></div>


<p>Next, it's a good idea to specify a <code>chroot</code> folder in the zookeeper connection string to keep the future flexibility of sharing it with other tools or even another Kafka cluster. Recall that several Kafka brokers are considered to be part of the same cluster if they share the same location on a zookeeper ensemble. Zookeeper is super sensible to load and access latency, so sharing it betweeen many frameworks is not always a good idea.</p>
<div class="highlight"><pre><span></span>zookeeper.connect=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181/kafka
</pre></div>


<p>While we're on the subject of zookeeper, the two following can be handy. Note that setting a long timeout is not a magic solution to latency issues since it makes the detection and resolution of crashed brokers slower.</p>
<div class="highlight"><pre><span></span>zookeeper.connection.timeout.ms=6000
zookeeper.session.timeout.ms=6000
</pre></div>


<p>The broker log location can be specified as a comma-separated list of mount points. For higher throughput, one can specify several disks here.</p>
<div class="highlight"><pre><span></span>log.dirs=/some/location,/some/other-location-on-another-disk
</pre></div>


<p>The following parameter specifies the number of threads used during startup and  shut-down for cleaning up logs and getting to a stable state. Since they are only used at that moment, increasing it may speedup startup time (especially right after a major failure that requires lots' of cleanup) and should otherwise not impact the performance during the rest of the lifetime of the broker.</p>
<div class="highlight"><pre><span></span>num.recovery.threads.per.data.dir=2
</pre></div>


<p>In production, I would disable topic auto-creation, to make sure all topics are created with explictly chosen parameters. I would also tend to disable the deletion of topics: </p>
<div class="highlight"><pre><span></span>auto.create.topics.enable=false

delete.topic.enable=false
</pre></div>


<p>Default max message size is 1M. That setting can also be set per topic:</p>
<div class="highlight"><pre><span></span># this is overridable at topic creation time with --config max.message.bytes
#message.max.bytes=1000000
</pre></div>


<h3>Data retention</h3>
<p>Log retention is configured by time and/or by size. If both are specified, whichever condition is true first triggers a cleanup. Time-based retention can be specified in <code>hour</code>, <code>minutes</code> or <code>ms</code>, you should only specify one of those time period, though if you specify several, the smallest time granularity takes precedence. </p>
<p>Logs are sliced into segments of the max size or max duration specified in the last two paremeters below. </p>
<p><strong>Gotcha 1</strong>: Kafka is not going to clean up less than a full and past segment. This means that if you have a low traffic topic and set its retention to, say, a couple of hours, data might still take days to be cleaned up since we need to fill up a segment before cleaning it up.</p>
<p><strong>Gotach 2</strong>: relying on <code>log.segment.ms</code> implies that segments of all topic partitions are going to be rolled at approximatively the same moment, which might impact all broker sensibly of you have lot's of partitions and data.</p>
<div class="highlight"><pre><span></span><span class="c1"># this can be overriden at topic creation time with --config retention.ms</span>
log.retention.hours<span class="o">=</span><span class="m">168</span>

<span class="c1"># this can be overriden at topic creation time with --config retention.bytes</span>
log.retention.bytes<span class="o">=</span><span class="m">1073741824</span>

<span class="c1"># this is overridable at topic creation time with --config segment.bytes </span>
log.segment.bytes<span class="o">=</span><span class="m">268435456</span>

<span class="c1"># this is overridable at topic creation time with --config segment.ms</span>
log.segment.ms<span class="o">=</span><span class="m">123456</span>
</pre></div>


<p>Consumer have their offsets committed in Kafka now (except if your client handles them explicitly in some other way), so they are also subject to retention. The default is 1 day. If you have a low traffic topic that might receive less than one message per day, your consumers offsets would not get updated and could be removed from Kafka. Setting <code>offsets.retention.minutes</code> to a higher value should help in such case. </p>
<div class="highlight"><pre><span></span># keep consumer offset for two weeks
offsets.retention.minutes=20160
</pre></div>


<h3>Broker data availability parameters</h3>
<p>If a topic is replicated, all read and write operations are performed on the leader partition and all other replicas are just slave copies. Such slave replica is said to be "out-of-sync" if it lags behind the latest records available in its leader. </p>
<p>In case the leader crashes at a moment when all live replicas are out-of-sync, Kafka will by default not allow such "unclean" replicas to become the new leader since data could be lost and/or consumers could be confused about offset fuzzy business. If you would like to favour availability over data consistency, you can choose to allow such "unclean leader election". Note that you can specify this per topic as well. </p>
<div class="highlight"><pre><span></span># this is overridable at topic creation time with --config unclean.leader.election.enable
unclean.leader.election.enable=false
</pre></div>


<p>The following parameter is a similar availabiliy vs consistency tradeoff: data producers have the possibility to request that "all" partition replicas confirm the reception of a record before considering the write operation as successful (cf <code>acks</code> parameter below). In case some replicas are known to be out-of-sync, we know they are not going to provide such acknowledgment at the moment. The parameter below specifies the minimum number of replicas that must still be in sync such that we can consider that "all" replicas have confirmed the reception of a record.</p>
<div class="highlight"><pre><span></span># this is overridable at topic creation time with --config min.insync.replicas
min.insync.replicas=2
</pre></div>


<h3>Kafka producers</h3>
<p>Kafka producers and consumer are rich clients that are packed with features like batching, message routing, compression, retries... and all that gets to be parameterized as well :) </p>
<p>One key piece of information to keep in mind is that configuring producers and consumers makes sense when we code directly against their API, <strong>as well when we want to configure Kafka Connect, Kafka Stream, Flink Kafka connector, Spark Kafka connector and pretty much any java or scala component that relies on them</strong>, simply because, well, all their features still matter once they're wrapped in such tools.  </p>
<h4>Basic parameters</h4>
<p><code>bootstrap.servers</code> should specify a couple of kafka brokers. If at least one of them is still valid at the moment the connection happens, the client will then rely on Kafka service discovery to figure out all the others. </p>
<div class="highlight"><pre><span></span>bootstrap.servers=some-broker:9092.some-other-broker:9092
</pre></div>


<p>Pretty much all a broker knows about a record payload is that it's a key/value pair made of bytes. Serializers are used by the producer to convert java instances to such byte arrays. One possible choice here is to rely on <a href="https://github.com/confluentinc/schema-registry/blob/3.3.1-post/avro-serializer/src/main/java/io/confluent/kafka/serializers/KafkaAvroSerializer.java">Confluent's avro / schema registry serializer</a> to obtain avro records with a schema properly declared and versioned in the Conluent schema registry.</p>
<div class="highlight"><pre><span></span>value.serializer=some.class

key.serializer=some.class
</pre></div>


<p>Producers also optionally handle data compression for us. For maximum throughput, there is a tradeoff to be experimentally found between message size and cpu time spent (de)compressing it.</p>
<div class="highlight"><pre><span></span># compression codec. 
# &quot;none&quot;, &quot;snappy&quot;, &quot;lz4&quot; or &quot;gzip&quot;
compression.type=lz4
</pre></div>


<p>The following controls the maximum amount of time the client will wait for the response of a request.</p>
<div class="highlight"><pre><span></span>request.timeout.ms=30000
</pre></div>


<h4>Delivery guaranties</h4>
<p>Kafka producers perform retries for us! As many as we want. The <code>retries</code> parameter specifies the maximum amount of retries that will be attempted on a retry-able error (like, leader not available) and <code>retry.backoff.ms</code> specifies how long to wait between each attempt. </p>
<p>Note that as the producer keep on retrying while potentially also trying to send new traffic, pending messages can quickly occupy some space, so make sure <code>buffer.memory</code> is set properly. Finally, once the memory buffer is full (or if topic metadata are impossible to obtain at the moment), the producer will block during <code>max.block.ms</code> before blowing up. </p>
<p>I guess this is Kafka's take on back-pressure. </p>
<p>Really, if we care about data consistency, and assuming all upstreams components are behaving accordingly, blocking might be the best thing to do here. Well, blowing up might also be the way. Each case should be designed. </p>
<div class="highlight"><pre><span></span># this default to 0, unless you enabled idempotence
retries=2147483647

retry.backoff.ms=100

buffer.memory=33554432

max.block.ms=60000
</pre></div>


<p>As mentioned above on the section <code>min.insync.replica</code>, producer can specify the amount of required acknowledgments for a write to be considered successful. </p>
<div class="highlight"><pre><span></span># this defaults to 1, unless you enabled idempotence
acks=all
</pre></div>


<p>Idempotent producers is one of the awesome feature that Kafka folks gifted to the world in version 0.11. That is a long subject, though in a nutshell it guarantees that successfully written records are written exactly once to the brokers. Previously, due to some corner cases in the retry mechanism, some message could have ended-up being duplicated. </p>
<div class="highlight"><pre><span></span># Idempotent retries features of Kafka, introduced in 0.11, 
# Part of components enabing Kafka Streams exactly-once processing semantics.
enable.idempotence=false
</pre></div>


<h4>Output record batching</h4>
<p>Kafka producers also automatically batch our records together and send them asynchronously!</p>
<p>In case enough data is available when the producer sends data, it will pack them per batch of <code>batch.size</code> bytes. If less data is available it just sends what it has without waiting, unless <code>linger.ms</code> is set to a positive value, in which case it waits a bit to get a chance to pack a few more: </p>
<div class="highlight"><pre><span></span>batch.size=16384

linger.ms=0
</pre></div>


<p>By default, the producer will wait for a batch to be acknowledged, as specified by <code>acks</code> above, before sending the next batch. For potentially faster throughput, we set the following parameter to some value greater than one to specify the maximum amount of such un-acknowledged batches that we allow.</p>
<p><strong>Gotcha</strong>: setting this to anything else than 1 destroys the per-topic ordering guarantee of Kafka, simply because some in-flight batches might fail and be retried, while others might go through, in any order.</p>
<div class="highlight"><pre><span></span># If set enable.idempotence to true, it may not be greater than 5
#max.in.flight.requests.per.connection=1
</pre></div>


<h3>Kafka consummers</h3>
<h4>Basic parameters</h4>
<p>Kafka consumer group share the read load when reading from topics. </p>
<div class="highlight"><pre><span></span>group.id=&quot;my client&quot;
</pre></div>


<p>See producer discussion regarding broker endpoint and serialisers</p>
<div class="highlight"><pre><span></span>bootstrap.servers=some-broker:9092.some-other-broker:9092

value.deserializer=some.class

key.deserializer=some.class
</pre></div>


<p><code>fetch.min.bytes=1</code> means the consumer will start reading as early as at least one message of at least one byte is available. Forcing higher values here might lead to reading by larger chunks and relying on less network round trips. </p>
<p>In a similar fashion, <code>max.poll.records</code> specifies, well, the maximum number of message to fetch each time. </p>
<div class="highlight"><pre><span></span>fetch.min.bytes=1

max.poll.records=500
</pre></div>


<p>The following specifies where to start reading a topic when a consumer appears and does not have a previous offset to start from. This is the typical situation that happens when a new consumer group is created, although it can also be relevant if that offset existed in the past but disappeared due to the <code>offsets.retention.minutes</code> parameter mentioned above. <code>latest</code> will make the consumer tail the log while <code>earliest</code> will (re)-start from the beginning.</p>
<div class="highlight"><pre><span></span>auto.offset.reset=earliest
</pre></div>


<p>Consumer offsets get committed by default to a Kafka topics. That is a reasonnable default, though sometimes you might prefer to handle them yourself. For example this <a href="http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/">blog post by Guru Medasani shows how to commit offsets with the processed data in Hbase</a>.</p>
<div class="highlight"><pre><span></span>enable.auto.commit=true
</pre></div>


<h4>Group robustness and record delivery guarantees</h4>
<p>A consumer instance needs to be considered alive to remain part of a consumer group. If it fails to emit hearbeats for more than <code>session.timeout.ms</code>, it gets kicked out and a group rebalance happens. </p>
<div class="highlight"><pre><span></span>heartbeat.interval.ms=3000

session.timeout.ms=10000
</pre></div>


<p>The Kafka consumer refreshes its knowledge of the metadata describing a topic at fixed interval, as defined below. Little known fact: if your consomer start consuming from a topic <em>before you create it</em> (it's not going to consumme much, is it?), maybe because some consuming client got deployed a bit too early, it will block, then retry discovering the location of the relevant partition after that period as well. </p>
<div class="highlight"><pre><span></span>metadata.max.age.ms=300000
</pre></div>


<p>Kafka 0.11 introduced so-called <em>transactions</em>. Essentially, they try to mimic the <em>read committed</em> isolation feature of ACID transactions by allowing a producer to mark a set of written records, typically accross several topics, as part of the same atomic write operation. Kafka consumer will ignore that feature by default, unless they are configured with <code>isolation.level=read.committed</code>, in which case any record that is not part of a committed transaction gets discarded. </p>
<p>Note that this does <strong>not</strong> achieve atomic read: this is an all-or-nothing <em>write</em> operation: from the read side, there is no way to have an all-or-nothing mechanism. </p>
<div class="highlight"><pre><span></span>isolation.level=read_uncommitted
</pre></div>


<h1>Sources:</h1>
<p>Most of the content above has been heavily inspired from the book chapters and blog posts below.</p>
<p>Kafka, the definitive guide - Gwen Shapira, Todd Palino,  Neha Narkhede:</p>
<ul>
<li><a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch02.html#installing_kafka">chapter 2: installing Kafka</a></li>
<li><a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch03.html#writing_messages_to_kafka">chapter 3: Kafka producers</a></li>
<li><a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch04.html">chapter 4: Kafka consumers</a></li>
<li><a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch06.html#reliable_data_delivery">chapter 6: Reliable data delivery</a></li>
</ul>
<p><a href="https://www.udemy.com/apache-kafka-series-setup-administration-in-production/">Apache Kafka setup series, Kafka setup and administration (Udemy class) - Stephane Maarek</a></p>
<p>Confluent blog: </p>
<ul>
<li><a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it">Exactly-once Semantics are Possible: Here’s How Kafka Does it - Neha Narkhede</a></li>
<li><a href="https://www.confluent.io/blog/transactions-apache-kafka/">Transactions in Apache Kafka - Apurva Mehta, Jason Gustafson</a></li>
</ul>
<p><a href="http://kafka.apache.org/documentation/#configuration">Kafka configuration documentation on kafka.org</a></p>
<p><a href="https://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html">Kafka producer Javadoc</a></p>
<p><a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_ha.html">Configuring High Availability and Consistency for Kafka | 3.0.x | Cloudera Documentation</a></p>
	<div id="disqus_thread"></div>
		<script type="text/javascript">
			var disqus_shortname = 'svend-blog';
			(function() {
	 			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	 			dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	 			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	 		})();
		</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</body>
</html>